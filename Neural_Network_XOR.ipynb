{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample of xor table\n",
    "Training_data = [\n",
    "    {'inputs': [0, 1],'target': [1]},\n",
    "    {'inputs': [1, 0],'target': [1]},\n",
    "    {'inputs': [0, 0],'target': [0]},\n",
    "    {'inputs': [1, 1],'target': [0]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test1 for 0 and 0 = [[0.02986487]]\n",
      "Test2 for 0 and 1 = [[0.96529992]]\n",
      "Test3 for 1 and 0 = [[0.96328172]]\n",
      "Test4 for 1 and 1 = [[0.04276897]]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(object):\n",
    "    #Step 1: prepare data\n",
    "    def __init__(self, input_layer, hidden_layer, output_layer):\n",
    "        #Prepare a simple nn with 3 layers\n",
    "        self.i_nodes = input_layer\n",
    "        self.h_nodes = hidden_layer\n",
    "        self.o_nodes = output_layer\n",
    "\n",
    "        #Sigmoid activation function\n",
    "        self.sigmoid = np.vectorize(lambda x: 1 / (1 + math.exp(-x)))\n",
    "        \n",
    "        #Derivative of sigmoid\n",
    "        self.dsigmoid = np.vectorize(lambda x: x * (1 - x))\n",
    "\n",
    "        #Learning rate (size of the corrective steps)\n",
    "        self.learning_rate = 0.1\n",
    "\n",
    "        #Generate random weights of input to hidden layer connections\n",
    "        self.weights_ih = np.random.uniform(-1, 1, (self.h_nodes, self.i_nodes))\n",
    "        #Generate random weights of hidden to output layer connections\n",
    "        self.weights_ho = np.random.uniform(-1, 1, (self.o_nodes, self.h_nodes))\n",
    "\n",
    "        #Generate random biases of the hidden layer\n",
    "        self.bias_h = np.random.rand(self.h_nodes, 1)\n",
    "        #Generate random biases of the output layer\n",
    "        self.bias_o = np.random.rand(self.o_nodes, 1)\n",
    "\n",
    "    #Step 2: Apply feedforward\n",
    "    def feedforward(self, input_array):\n",
    "\n",
    "        #Transform the input array into a vector\n",
    "        input_v = np.asmatrix(input_array)\n",
    "        input_v = np.reshape(input_v, (self.i_nodes, 1))\n",
    "\n",
    "        #Calculate new hidden layer results\n",
    "        hidden = self.weights_ih.dot(input_v)\n",
    "        hidden = hidden + self.bias_h\n",
    "\n",
    "        #Run the hidden results through sigmoid\n",
    "        hidden = self.sigmoid(hidden)\n",
    "\n",
    "        #Calculate new output layer results\n",
    "        output = self.weights_ho.dot(hidden)\n",
    "        output = output + self.bias_o\n",
    "\n",
    "        #Run the output results through sigomid\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    #Step 3: Do back propagation and train data n times\n",
    "    def train(self, input_array, answers_array):\n",
    "\n",
    "        #Transform the input array into a vector\n",
    "        input_v = np.asmatrix(input_array)\n",
    "        input_v = np.reshape(input_v, (self.i_nodes, 1))\n",
    "\n",
    "        #Calculate hidden layer results\n",
    "        hidden = self.weights_ih.dot(input_v)\n",
    "        hidden = hidden + self.bias_h\n",
    "        #Run hidden result through sigmoid\n",
    "        hidden = self.sigmoid(hidden)\n",
    "        \n",
    "        #Calculate output layer results\n",
    "        output = self.weights_ho.dot(hidden)\n",
    "        output = output + self.bias_o\n",
    "        #Run output result through sigmoid\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        #Transform the answer array into a vector\n",
    "        answers_vector = np.asmatrix(answers_array)\n",
    "        answers_vector = np.reshape(answers_vector, (self.o_nodes, 1))\n",
    "\n",
    "        #Calculate the error using ...\n",
    "        output_errors = answers_vector - output\n",
    "\n",
    "        #Calculate output gradient\n",
    "        #Formula: LR * E * d (final_output).hidden_layer_values_transposed\n",
    "        \n",
    "        #1. Calculate derivative of output through the derivative sigmoid function\n",
    "        derivative_output = self.dsigmoid(output)\n",
    "        \n",
    "        #2. Multiply d and E\n",
    "        weights_gradient = np.multiply(derivative_output, output_errors)\n",
    "        \n",
    "        #3. Multiply d and E with learning rate\n",
    "        weights_gradient = weights_gradient * self.learning_rate\n",
    "\n",
    "        hidden_t = hidden.transpose()\n",
    "        weights_ho_deltas = weights_gradient.dot(hidden_t)\n",
    "        \n",
    "        #Adjust the weights from hidden to output\n",
    "        self.weights_ho = self.weights_ho + weights_ho_deltas\n",
    "        self.bias_o = self.bias_o + weights_gradient\n",
    "\n",
    "        #Calculate the hidden layer error\n",
    "        weights_ho_t = self.weights_ho.transpose()\n",
    "        hidden_errors = weights_ho_t.dot(output_errors)\n",
    "\n",
    "        #Calculate hidden gradient\n",
    "        #Formula: LR * E * d (final_output).hidden_layer_values_transposed\n",
    "        \n",
    "        #1. Calculate derivative of hidden through the derivative sigmoid function\n",
    "        derivative_output = self.dsigmoid(hidden)\n",
    "\n",
    "        #2. Multiply d and E\n",
    "        hidden_gradient = np.multiply(derivative_output, hidden_errors)\n",
    "        \n",
    "        #3. Multiply d and E with learning rate\n",
    "        hidden_gradient = hidden_gradient * self.learning_rate\n",
    "        \n",
    "        inputs_t = input_v.transpose()\n",
    "        weights_ih_d = hidden_gradient.dot(inputs_t)\n",
    "\n",
    "        #Adjust the weights from input to hidden\n",
    "        self.weights_ih = self.weights_ih + weights_ih_d\n",
    "        self.bias_h = self.bias_h + hidden_gradient\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Define 2 for input layer, 4 for hidden layer, 1 for output layer\n",
    "    nn = NeuralNetwork(2, 4, 1)\n",
    "\n",
    "    #Train the data using neural network n (20,000) times\n",
    "    for i in range(20000):\n",
    "        el = random.choice(Training_data)\n",
    "        nn.train(el['inputs'], el['target'])\n",
    "    \n",
    "    #Test the model after training and print the results\n",
    "    print('Test1 for 0 and 0 = {}'.format(nn.feedforward([0, 0])))\n",
    "    print('Test2 for 0 and 1 = {}'.format(nn.feedforward([0, 1])))\n",
    "    print('Test3 for 1 and 0 = {}'.format(nn.feedforward([1, 0])))\n",
    "    print('Test4 for 1 and 1 = {}'.format(nn.feedforward([1, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
